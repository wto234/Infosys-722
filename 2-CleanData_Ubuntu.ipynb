{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import Libraries\n",
    "import findspark\n",
    "findspark.init('./spark-2.1.1-bin-hadoop2.7')\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession \n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SQLContext\n",
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc = SparkContext()\n",
    "conf = SparkConf().setAppName(\"pySparkMerge\").setMaster(\"local\")\n",
    "sc = SparkContext.getOrCreate(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##CREATING \"SPARKSESSION\" from existing \"SPARKCONTEXT\"\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = sqlContext.sparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = spark.read.csv('./KaggleV2-chart.csv/', header=True)\n",
    "df00 = spark.read.csv('./KaggleV2-chart.csv/', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(110527, 14)\n",
      "root\n",
      " |-- PatientId: string (nullable = true)\n",
      " |-- Scholarship: string (nullable = true)\n",
      " |-- Hipertension: string (nullable = true)\n",
      " |-- Diabetes: string (nullable = true)\n",
      " |-- Alcoholism: string (nullable = true)\n",
      " |-- Handcap: string (nullable = true)\n",
      " |-- AppointmentID: string (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- ScheduledDay: string (nullable = true)\n",
      " |-- AppointmentDay: string (nullable = true)\n",
      " |-- Age: string (nullable = true)\n",
      " |-- Neighbourhood: string (nullable = true)\n",
      " |-- SMS_received: string (nullable = true)\n",
      " |-- No-show: string (nullable = true)\n",
      "\n",
      "None\n",
      "+---------+-----------+------------+--------+----------+-------+-------------+------+------------+--------------+---+-------------+------------+-------+\n",
      "|PatientId|Scholarship|Hipertension|Diabetes|Alcoholism|Handcap|AppointmentID|Gender|ScheduledDay|AppointmentDay|Age|Neighbourhood|SMS_received|No-show|\n",
      "+---------+-----------+------------+--------+----------+-------+-------------+------+------------+--------------+---+-------------+------------+-------+\n",
      "|        0|          0|           0|       0|         0|      0|            0|     0|           0|             0|  0|            0|          59|      0|\n",
      "+---------+-----------+------------+--------+----------+-------+-------------+------+------------+--------------+---+-------------+------------+-------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print((df0.count(), len(df0.columns)))\n",
    "print( df0.printSchema() )\n",
    "## CHECK ANY MISSING VALUSE\n",
    "### Get count of both null and missing values in pyspark\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "print( df0.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df0.columns]).show() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df0.select('SMS_received').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+\n",
      "|  PatientId|Scholarship|Hipertension|Diabetes|Alcoholism|Handcap|AppointmentID|Gender|        ScheduledDay|      AppointmentDay|Age|  Neighbourhood|SMS_received|No-show|\n",
      "+-----------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+\n",
      "|2.98725E+13|          0|           1|       0|         0|      0|      5642903|     F|2016-04-29T18:38:08Z|2016-04-29T00:00:00Z| 62|JARDIM DA PENHA|           0|     No|\n",
      "|5.58998E+14|          0|           0|       0|         0|      0|      5642503|     M|2016-04-29T16:08:27Z|2016-04-29T00:00:00Z| 56|JARDIM DA PENHA|           0|     No|\n",
      "+-----------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+\n",
      "only showing top 2 rows\n",
      "\n",
      "root\n",
      " |-- PatientId: float (nullable = true)\n",
      " |-- Scholarship: integer (nullable = true)\n",
      " |-- Hipertension: integer (nullable = true)\n",
      " |-- Diabetes: integer (nullable = true)\n",
      " |-- Alcoholism: integer (nullable = true)\n",
      " |-- Handcap: integer (nullable = true)\n",
      " |-- AppointmentID: integer (nullable = true)\n",
      " |-- Gender: string (nullable = true)\n",
      " |-- ScheduledDay: string (nullable = true)\n",
      " |-- AppointmentDay: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Neighbourhood: string (nullable = true)\n",
      " |-- SMS_received: float (nullable = true)\n",
      " |-- No-show: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df0.printSchema()\n",
    "df0.show(2)\n",
    "from pyspark.sql.types import IntegerType, FloatType\n",
    "df0 = df0.withColumn(\"PatientId\", df0[\"PatientId\"].cast(FloatType()))\n",
    "df0 = df0.withColumn(\"Scholarship\", df0[\"Scholarship\"].cast(IntegerType()))\n",
    "df0 = df0.withColumn(\"Hipertension\", df0[\"Hipertension\"].cast(IntegerType()))\n",
    "df0 = df0.withColumn(\"Diabetes\", df0[\"Diabetes\"].cast(IntegerType()))\n",
    "df0 = df0.withColumn(\"Alcoholism\", df0[\"Alcoholism\"].cast(IntegerType()))\n",
    "df0 = df0.withColumn(\"Handcap\", df0[\"Handcap\"].cast(IntegerType()))\n",
    "df0 = df0.withColumn(\"AppointmentID\", df0[\"AppointmentID\"].cast(IntegerType()))\n",
    "df0 = df0.withColumn(\"Age\", df0[\"Age\"].cast(IntegerType()))\n",
    "df0 = df0.withColumn(\"SMS_received\", df0[\"SMS_received\"].cast(FloatType()))\n",
    "df0.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('PatientId', 'float'),\n",
       " ('Scholarship', 'int'),\n",
       " ('Hipertension', 'int'),\n",
       " ('Diabetes', 'int'),\n",
       " ('Alcoholism', 'int'),\n",
       " ('Handcap', 'int'),\n",
       " ('AppointmentID', 'int'),\n",
       " ('Gender', 'string'),\n",
       " ('ScheduledDay', 'string'),\n",
       " ('AppointmentDay', 'string'),\n",
       " ('Age', 'int'),\n",
       " ('Neighbourhood', 'string'),\n",
       " ('SMS_received', 'float'),\n",
       " ('No-show', 'string')]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'int'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.dtypes\n",
    "dict(df0.dtypes)['Scholarship']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Gender', 'ScheduledDay', 'AppointmentDay', 'Neighbourhood', 'No-show']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Append STRING column\n",
    "# for c in df0.columns:\n",
    "#     if( (dict(df0.dtypes)[c] == 'int') | (dict(df0.dtypes)[c] == 'float')):\n",
    "#         print(c)\n",
    "omit = []\n",
    "for c in df0.columns:\n",
    "    if( (dict(df0.dtypes)[c] == 'string')):\n",
    "        omit.append(c)\n",
    "# omit.append('PatientId')\n",
    "omit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Remove the null value from SMS_received\n",
    "# df0.withColumn('SMS_received',F.when(F.isnan(F.col('SMS_received'))|F.col('SMS_received').isNull(),0).otherwise(F.col('SMS_received'))\n",
    "\n",
    "# df0 = df0.withColumn('SMS_received',F.when(F.col('SMS_received').isNull(),0).otherwise(F.col('SMS_received')))\n",
    "# when(isnan(c) | col(c).isNull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.functions import col, avg\n",
    "# def fill_with_mean(df, exclude=set()): \n",
    "#     for c in df0.columns:\n",
    "#         if( (dict(df0.dtypes)[c] == 'int') | (dict(df0.dtypes)[c] == 'float')):\n",
    "#             stats = df0.agg(*(\n",
    "#                 avg(c).alias(c) \n",
    "#                 ))        \n",
    "#     return df0.na.fill(stats.first().asDict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### WORKING ONE , I made it applied mean only for SMS_received\n",
    "from pyspark.sql.functions import col, avg\n",
    "def fill_with_mean(df, exclude=set()): \n",
    "#     print(exclude)\n",
    "    stats = df.agg(*(\n",
    "        avg(c).alias(c) for c in df.columns if c not in exclude\n",
    "    ))\n",
    "    return df.na.fill(stats.first().asDict())\n",
    "\n",
    "# fill_with_mean(df_data, [\"id\", \"date\"])\n",
    "# df0 = fill_with_mean(df0, omit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = fill_with_mean(df0, omit)\n",
    "\n",
    "## df1 = df0.rdd.map(lambda x: x.na.fill(meanOrMode(x)))\n",
    "# df1.count()\n",
    "\n",
    "## summary4 = df0.isnull().any()\n",
    "# df0.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df0.select('PatientId').show(3)\n",
    "\n",
    "# df0.withColumn('PatientId', df00.PatientId).show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+--------+----------+-------+-------------+------+------------+--------------+---+-------------+------------+-------+\n",
      "|PatientId|Scholarship|Hipertension|Diabetes|Alcoholism|Handcap|AppointmentID|Gender|ScheduledDay|AppointmentDay|Age|Neighbourhood|SMS_received|No-show|\n",
      "+---------+-----------+------------+--------+----------+-------+-------------+------+------------+--------------+---+-------------+------------+-------+\n",
      "|        0|          0|           0|       0|         0|      0|            0|     0|           0|             0|  0|            0|           0|      0|\n",
      "+---------+-----------+------------+--------+----------+-------+-------------+------+------------+--------------+---+-------------+------------+-------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "## CHECK ANY MISSING VALUSE\n",
    "### Get count of both null and missing values in pyspark\n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "print( df0.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df0.columns]).show() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----+\n",
      "|AppointmentID|count|\n",
      "+-------------+-----+\n",
      "|      5642643|    1|\n",
      "|      5637496|    1|\n",
      "|      5629170|    1|\n",
      "|      5615023|    1|\n",
      "|      5638919|    1|\n",
      "|      5637655|    1|\n",
      "|      5637879|    1|\n",
      "|      5642171|    1|\n",
      "|      5548957|    1|\n",
      "|      5642795|    1|\n",
      "|      5640781|    1|\n",
      "|      5640214|    1|\n",
      "|      5638301|    1|\n",
      "|      5521710|    1|\n",
      "|      5522956|    1|\n",
      "|      5626229|    1|\n",
      "|      5630294|    1|\n",
      "|      5637310|    1|\n",
      "|      5683766|    1|\n",
      "|      5486000|    1|\n",
      "+-------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Check all appointment ids are unique -- value_counts() \n",
    "print(df0.groupBy('AppointmentID').count().orderBy('count').show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------+\n",
      "|min(Age)|max(Age)|\n",
      "+--------+--------+\n",
      "|      -1|     115|\n",
      "+--------+--------+\n",
      "\n",
      "None\n",
      "+-------+--------------------+-------------------+-------------------+-------------------+--------------------+--------------------+-----------------+------+--------------------+--------------------+------------------+-------------+-------------------+-------+\n",
      "|summary|           PatientId|        Scholarship|       Hipertension|           Diabetes|          Alcoholism|             Handcap|    AppointmentID|Gender|        ScheduledDay|      AppointmentDay|               Age|Neighbourhood|       SMS_received|No-show|\n",
      "+-------+--------------------+-------------------+-------------------+-------------------+--------------------+--------------------+-----------------+------+--------------------+--------------------+------------------+-------------+-------------------+-------+\n",
      "|  count|              110527|             110527|             110527|             110527|              110527|              110527|           110527|110527|              110527|              110527|            110527|       110527|             110527| 110527|\n",
      "|   mean|1.474962664047398...|0.09826558216544373| 0.1972459218109602|0.07186479321794674|0.030399811810688793|0.022247957512643968|5675305.123426855|  null|                null|                null| 37.08887421173107|         null|0.32102509324505785|   null|\n",
      "| stddev|2.560949221791547...| 0.2976747541093073|0.39792134994708606|0.25826507350746536|  0.1716855554142438|  0.1615427258142786|  71295.751539677|  null|                null|                null|23.110204963682676|         null|0.46674789890209833|   null|\n",
      "|    min|             39200.0|                  0|                  0|                  0|                   0|                   0|          5030230|     F|2015-11-10T07:13:56Z|2016-04-29T00:00:00Z|                -1|    AEROPORTO|                0.0|     No|\n",
      "|    max|          9.99982E14|                  1|                  1|                  1|                   1|                   4|          5790484|     M|2016-06-08T20:07:23Z|2016-06-08T00:00:00Z|               115|   VILA RUBIM|                1.0|    Yes|\n",
      "+-------+--------------------+-------------------+-------------------+-------------------+--------------------+--------------------+-----------------+------+--------------------+--------------------+------------------+-------------+-------------------+-------+\n",
      "\n",
      "None\n",
      "+--------+--------+\n",
      "|min(Age)|max(Age)|\n",
      "+--------+--------+\n",
      "|       0|     115|\n",
      "+--------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove negative value for Age \n",
    "df0.createOrReplaceTempView(\"df0_tbl\")\n",
    "\n",
    "query = '''SELECT min(Age), max(Age) FROM df0_tbl '''\n",
    "print(spark.sql(query).show(5))\n",
    "print(df0.describe().show())\n",
    "\n",
    "df0 = df0.withColumn('Age', when(col('Age') != -1,col('Age')) )\n",
    "\n",
    "# df= df.withColumn('foo', when(col('foo') != 'empty-value',col('foo)))\n",
    "df0.createOrReplaceTempView(\"df0_tbl\")\n",
    "\n",
    "query = '''SELECT min(Age), max(Age) FROM df0_tbl '''\n",
    "spark.sql(query).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df0.columns\n",
    "# df0['Gender'] = df0['Gender'].map({'M':0,'F':1})\n",
    "# df0['No-show'] = df0['No-show'].map({'Yes':1, 'No':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+\n",
      "|    PatientId|Scholarship|Hipertension|Diabetes|Alcoholism|Handcap|AppointmentID|Gender|        ScheduledDay|      AppointmentDay|Age|  Neighbourhood|SMS_received|No-show|\n",
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+\n",
      "|   2.98725E13|          0|           1|       0|         0|      0|      5642903|     1|2016-04-29T18:38:08Z|2016-04-29T00:00:00Z| 62|JARDIM DA PENHA|         0.0|      0|\n",
      "|5.58998013E14|          0|           0|       0|         0|      0|      5642503|     0|2016-04-29T16:08:27Z|2016-04-29T00:00:00Z| 56|JARDIM DA PENHA|         0.0|      0|\n",
      "|4.26296004E12|          0|           0|       0|         0|      0|      5642549|     1|2016-04-29T16:19:04Z|2016-04-29T00:00:00Z| 62|  MATA DA PRAIA|         0.0|      0|\n",
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## TRANSFORM FEATURES - ENCODE DATA\n",
    "\n",
    "# Assign target and gender feature as binaries\n",
    "# For 'No-show': Yes = 1, No = 0\n",
    "# For 'Gender: F = 1, M = 0\n",
    "from functools import reduce\n",
    "\n",
    "cols = ['Gender']\n",
    "df0 = reduce(lambda df, c: df.withColumn(c, F.when(df[c] == 'M', 0).otherwise(1)), cols, df0)\n",
    "\n",
    "cols = ['No-show']\n",
    "df0 = reduce(lambda df, c: df.withColumn(c, F.when(df[c] == 'No', 0).otherwise(1)), cols, df0)\n",
    "\n",
    "df0.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pyspark.sql.functions.split(df0['ScheduledDay'], 'T').collect().show(2)\n",
    "split_col = pyspark.sql.functions.split(df0['ScheduledDay'], 'T')\n",
    "df0 = df0.withColumn('ScheduledDay_Day', split_col.getItem(0))\n",
    "df0 = df0.withColumn('ScheduledDay_Time', split_col.getItem(1))\n",
    "\n",
    "split_col = pyspark.sql.functions.split(df0['AppointmentDay'], 'T')\n",
    "df0 = df0.withColumn('AppointmentDay_Day', split_col.getItem(0))\n",
    "df0 = df0.withColumn('AppointmentDay_Time', split_col.getItem(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+----------------+-----------------+------------------+-------------------+\n",
      "|    PatientId|Scholarship|Hipertension|Diabetes|Alcoholism|Handcap|AppointmentID|Gender|        ScheduledDay|      AppointmentDay|Age|  Neighbourhood|SMS_received|No-show|ScheduledDay_Day|ScheduledDay_Time|AppointmentDay_Day|AppointmentDay_Time|\n",
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+----------------+-----------------+------------------+-------------------+\n",
      "|   2.98725E13|          0|           1|       0|         0|      0|      5642903|     1|2016-04-29T18:38:08Z|2016-04-29T00:00:00Z| 62|JARDIM DA PENHA|         0.0|      0|      2016-04-29|        18:38:08Z|        2016-04-29|          00:00:00Z|\n",
      "|5.58998013E14|          0|           0|       0|         0|      0|      5642503|     0|2016-04-29T16:08:27Z|2016-04-29T00:00:00Z| 56|JARDIM DA PENHA|         0.0|      0|      2016-04-29|        16:08:27Z|        2016-04-29|          00:00:00Z|\n",
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+----------------+-----------------+------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+\n",
      "|AppointmentDay_Time| count|\n",
      "+-------------------+------+\n",
      "|          00:00:00Z|110527|\n",
      "+-------------------+------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# No 'time stamp' in 'AppointmentDay' features\n",
    "print(df0.groupBy('AppointmentDay_Time').count().orderBy('count').show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+----------------+-----------------+------------------+-------------------+\n",
      "|    PatientId|Scholarship|Hipertension|Diabetes|Alcoholism|Handcap|AppointmentID|Gender|        ScheduledDay|      AppointmentDay|Age|  Neighbourhood|SMS_received|No-show|ScheduledDay_Day|ScheduledDay_Time|AppointmentDay_Day|AppointmentDay_Time|\n",
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+----------------+-----------------+------------------+-------------------+\n",
      "|   2.98725E13|          0|           1|       0|         0|      0|      5642903|     1|2016-04-29T18:38:08Z|2016-04-29T00:00:00Z| 62|JARDIM DA PENHA|         0.0|      0|      2016-04-29|        18:38:08Z|        2016-04-29|          00:00:00Z|\n",
      "|5.58998013E14|          0|           0|       0|         0|      0|      5642503|     0|2016-04-29T16:08:27Z|2016-04-29T00:00:00Z| 56|JARDIM DA PENHA|         0.0|      0|      2016-04-29|        16:08:27Z|        2016-04-29|          00:00:00Z|\n",
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+----------------+-----------------+------------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Z trailed at the value end\n",
    "# from pyspark.sql.functions import substring, length\n",
    "# df1 = df0.withColumn(\"ScheduledDay_Time\",substring(col(\"ScheduledDay_Time\"),0,length(col(\"ScheduledDay_Time\"))-1))\n",
    "df0 = df0.withColumn(\"ScheduledDay_Time\", df0[\"ScheduledDay_Time\"].substr(0,8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature for hour of the day from 'scheduledDay'\n",
    "df0 = df0.withColumn('ScheduledDay_Hours', df0['ScheduledDay_Time'].substr(0,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+----------------+-----------------+------------------+-------------------+------------------+\n",
      "|    PatientId|Scholarship|Hipertension|Diabetes|Alcoholism|Handcap|AppointmentID|Gender|        ScheduledDay|      AppointmentDay|Age|  Neighbourhood|SMS_received|No-show|ScheduledDay_Day|ScheduledDay_Time|AppointmentDay_Day|AppointmentDay_Time|ScheduledDay_Hours|\n",
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+----------------+-----------------+------------------+-------------------+------------------+\n",
      "|   2.98725E13|          0|           1|       0|         0|      0|      5642903|     1|2016-04-29T18:38:08Z|2016-04-29T00:00:00Z| 62|JARDIM DA PENHA|         0.0|      0|      2016-04-29|         18:38:08|        2016-04-29|          00:00:00Z|                18|\n",
      "|5.58998013E14|          0|           0|       0|         0|      0|      5642503|     0|2016-04-29T16:08:27Z|2016-04-29T00:00:00Z| 56|JARDIM DA PENHA|         0.0|      0|      2016-04-29|         16:08:27|        2016-04-29|          00:00:00Z|                16|\n",
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+----------------+-----------------+------------------+-------------------+------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df0.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PatientId: float (nullable = false)\n",
      " |-- Scholarship: integer (nullable = true)\n",
      " |-- Hipertension: integer (nullable = true)\n",
      " |-- Diabetes: integer (nullable = true)\n",
      " |-- Alcoholism: integer (nullable = true)\n",
      " |-- Handcap: integer (nullable = true)\n",
      " |-- AppointmentID: integer (nullable = true)\n",
      " |-- Gender: integer (nullable = false)\n",
      " |-- ScheduledDay: string (nullable = true)\n",
      " |-- AppointmentDay: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- Neighbourhood: string (nullable = true)\n",
      " |-- SMS_received: float (nullable = false)\n",
      " |-- No-show: integer (nullable = false)\n",
      " |-- ScheduledDay_Day: date (nullable = true)\n",
      " |-- ScheduledDay_Time: string (nullable = true)\n",
      " |-- AppointmentDay_Day: date (nullable = true)\n",
      " |-- AppointmentDay_Time: string (nullable = true)\n",
      " |-- ScheduledDay_Hours: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert to 'datetime' features dateType conversion\n",
    "from pyspark.sql.types import DateType\n",
    "df0 = df0.withColumn(\"ScheduledDay_Day\",df0['ScheduledDay_Day'].cast(DateType()))\n",
    "df0 = df0.withColumn(\"AppointmentDay_Day\",df0['AppointmentDay_Day'].cast(DateType()))\n",
    "df0.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+----------------+-----------------+------------------+-------------------+------------------+-----------------+-------------------+\n",
      "|    PatientId|Scholarship|Hipertension|Diabetes|Alcoholism|Handcap|AppointmentID|Gender|        ScheduledDay|      AppointmentDay|Age|  Neighbourhood|SMS_received|No-show|ScheduledDay_Day|ScheduledDay_Time|AppointmentDay_Day|AppointmentDay_Time|ScheduledDay_Hours|ScheduledDay_Date|AppointmentDay_Date|\n",
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+----------------+-----------------+------------------+-------------------+------------------+-----------------+-------------------+\n",
      "|   2.98725E13|          0|           1|       0|         0|      0|      5642903|     1|2016-04-29T18:38:08Z|2016-04-29T00:00:00Z| 62|JARDIM DA PENHA|         0.0|      0|      2016-04-29|         18:38:08|        2016-04-29|          00:00:00Z|                18|               29|                 29|\n",
      "|5.58998013E14|          0|           0|       0|         0|      0|      5642503|     0|2016-04-29T16:08:27Z|2016-04-29T00:00:00Z| 56|JARDIM DA PENHA|         0.0|      0|      2016-04-29|         16:08:27|        2016-04-29|          00:00:00Z|                16|               29|                 29|\n",
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+----------------+-----------------+------------------+-------------------+------------------+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create feature for date for scheduled date 'DD'\n",
    "df0 = df0.withColumn('ScheduledDay_Date', df0['ScheduledDay_Day'].substr(9,10))\n",
    "df0 = df0.withColumn('AppointmentDay_Date', df0['AppointmentDay_Day'].substr(9,10))\n",
    "df0.show(2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+----------------+-----------------+------------------+-------------------+------------------+-----------------+-------------------+--------+\n",
      "|    PatientId|Scholarship|Hipertension|Diabetes|Alcoholism|Handcap|AppointmentID|Gender|        ScheduledDay|      AppointmentDay|Age|  Neighbourhood|SMS_received|No-show|ScheduledDay_Day|ScheduledDay_Time|AppointmentDay_Day|AppointmentDay_Time|ScheduledDay_Hours|ScheduledDay_Date|AppointmentDay_Date|day_diff|\n",
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+----------------+-----------------+------------------+-------------------+------------------+-----------------+-------------------+--------+\n",
      "|   2.98725E13|          0|           1|       0|         0|      0|      5642903|     1|2016-04-29T18:38:08Z|2016-04-29T00:00:00Z| 62|JARDIM DA PENHA|         0.0|      0|      2016-04-29|         18:38:08|        2016-04-29|          00:00:00Z|                18|               29|                 29|       0|\n",
      "|5.58998013E14|          0|           0|       0|         0|      0|      5642503|     0|2016-04-29T16:08:27Z|2016-04-29T00:00:00Z| 56|JARDIM DA PENHA|         0.0|      0|      2016-04-29|         16:08:27|        2016-04-29|          00:00:00Z|                16|               29|                 29|       0|\n",
      "|4.26296004E12|          0|           0|       0|         0|      0|      5642549|     1|2016-04-29T16:19:04Z|2016-04-29T00:00:00Z| 62|  MATA DA PRAIA|         0.0|      0|      2016-04-29|         16:19:04|        2016-04-29|          00:00:00Z|                16|               29|                 29|       0|\n",
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+---------------+------------+-------+----------------+-----------------+------------------+-------------------+------------------+-----------------+-------------------+--------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# # Calculate time difference from 'ScheduleDay' to 'AppointmentDay'  \n",
    "# Extract only the days - output will be a rouned number \n",
    "# df0['day_diff'] = df0['AppointmentDay_Day'] - df0['ScheduledDay_Day']\n",
    "# df0.info()\n",
    "from pyspark.sql.functions import datediff, to_date, lit, unix_timestamp\n",
    "df0 = df0.withColumn(\"day_diff\",datediff('AppointmentDay_Day','ScheduledDay_Day'))\n",
    "df0.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|          day_diff|\n",
      "+------------------+\n",
      "|            110527|\n",
      "|10.183701719941734|\n",
      "|15.254996224106186|\n",
      "|                -6|\n",
      "|               179|\n",
      "+------------------+\n",
      "\n",
      "None\n",
      "+--------+\n",
      "|day_diff|\n",
      "+--------+\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       0|\n",
      "|       2|\n",
      "|       2|\n",
      "|       2|\n",
      "|       0|\n",
      "|       2|\n",
      "|       2|\n",
      "|       3|\n",
      "|       1|\n",
      "|       1|\n",
      "|       1|\n",
      "|       3|\n",
      "|       1|\n",
      "|       1|\n",
      "|       3|\n",
      "|       0|\n",
      "+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# outliers - negative date differences \n",
    "print(df0.describe().select(\"day_diff\").show()) \n",
    "print(df0.select('day_diff').show())\n",
    "# df.select(\"col\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+-------------+------------+-------+----------------+-----------------+------------------+-------------------+------------------+-----------------+-------------------+--------+\n",
      "|    PatientId|Scholarship|Hipertension|Diabetes|Alcoholism|Handcap|AppointmentID|Gender|        ScheduledDay|      AppointmentDay|Age|Neighbourhood|SMS_received|No-show|ScheduledDay_Day|ScheduledDay_Time|AppointmentDay_Day|AppointmentDay_Time|ScheduledDay_Hours|ScheduledDay_Date|AppointmentDay_Date|day_diff|\n",
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+-------------+------------+-------+----------------+-----------------+------------------+-------------------+------------------+-----------------+-------------------+--------+\n",
      "|   7.83927E12|          0|           0|       0|         0|      1|      5679978|     0|2016-05-10T10:51:53Z|2016-05-09T00:00:00Z| 38|  RESIST?NCIA|         0.0|      1|      2016-05-10|         10:51:53|        2016-05-09|          00:00:00Z|                10|               10|                 09|      -1|\n",
      "|   7.89629E12|          0|           0|       0|         0|      1|      5715660|     1|2016-05-18T14:50:41Z|2016-05-17T00:00:00Z| 19|SANTO ANT?NIO|         0.0|      1|      2016-05-18|         14:50:41|        2016-05-17|          00:00:00Z|                14|               18|                 17|      -1|\n",
      "|2.42523004E13|          0|           0|       0|         0|      0|      5664962|     1|2016-05-05T13:43:58Z|2016-05-04T00:00:00Z| 22|   CONSOLA??O|         0.0|      1|      2016-05-05|         13:43:58|        2016-05-04|          00:00:00Z|                13|               05|                 04|      -1|\n",
      "|   9.98232E14|          0|           0|       0|         0|      0|      5686628|     1|2016-05-11T13:49:20Z|2016-05-05T00:00:00Z| 81|SANTO ANT?NIO|         0.0|      1|      2016-05-11|         13:49:20|        2016-05-05|          00:00:00Z|                13|               11|                 05|      -6|\n",
      "| 3.7874801E12|          0|           0|       0|         0|      0|      5655637|     0|2016-05-04T06:50:57Z|2016-05-03T00:00:00Z|  7|   TABUAZEIRO|         0.0|      1|      2016-05-04|         06:50:57|        2016-05-03|          00:00:00Z|                06|               04|                 03|      -1|\n",
      "+-------------+-----------+------------+--------+----------+-------+-------------+------+--------------------+--------------------+---+-------------+------------+-------+----------------+-----------------+------------------+-------------------+------------------+-----------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# df0.select('day_diff')\n",
    "df0.where(df0.day_diff < 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----------+------------+--------+----------+-------+-------------+------+------------+--------------+---+-------------+------------+-------+----------------+-----------------+------------------+-------------------+------------------+-----------------+-------------------+--------+\n",
      "|PatientId|Scholarship|Hipertension|Diabetes|Alcoholism|Handcap|AppointmentID|Gender|ScheduledDay|AppointmentDay|Age|Neighbourhood|SMS_received|No-show|ScheduledDay_Day|ScheduledDay_Time|AppointmentDay_Day|AppointmentDay_Time|ScheduledDay_Hours|ScheduledDay_Date|AppointmentDay_Date|day_diff|\n",
      "+---------+-----------+------------+--------+----------+-------+-------------+------+------------+--------------+---+-------------+------------+-------+----------------+-----------------+------------------+-------------------+------------------+-----------------+-------------------+--------+\n",
      "+---------+-----------+------------+--------+----------+-------+-------------+------+------------+--------------+---+-------------+------------+-------+----------------+-----------------+------------------+-------------------+------------------+-----------------+-------------------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Replace with 'nan' so that they will be dropped later\n",
    "df0 = df0.withColumn('day_diff',F.when(df0['day_diff']<0,None).otherwise(F.col('day_diff')))\n",
    "\n",
    "# Check any negative value again\n",
    "df0.where(df0.day_diff < 0).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df0.columns\n",
    "# .select('df00.PatientId','df0.Scholarship','df0.Hipertension','df0.Diabetes','df0.Alcoholism','df0.Handcap','df0.AppointmentID','df0.Gender','df0.ScheduledDay','df0.AppointmentDay','df0.Age','df0.Neighbourhood','df0.SMS_received','df0.No-show','df0.ScheduledDay_Day','df0.ScheduledDay_Time','df0.AppointmentDay_Day','df0.AppointmentDay_Time','df0.ScheduledDay_Hours','df0.ScheduledDay_Date','df0.AppointmentDay_Date','df0.day_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df00.select('PatientId').collect()\n",
    "# df0.withColumn('pid', df00.select('PatientId').collect())\n",
    "\n",
    "# df0 = df0.alias('df0').join(df00.alias('df00'), df0.AppointmentID == df00.AppointmentID ).select('df00.PatientId','df0.Scholarship','df0.Hipertension','df0.Diabetes','df0.Alcoholism','df0.Handcap','df0.AppointmentID','df0.Gender','df0.ScheduledDay','df0.AppointmentDay','df0.Age','df0.Neighbourhood','df0.SMS_received','df0.No-show','df0.ScheduledDay_Day','df0.ScheduledDay_Time','df0.AppointmentDay_Day','df0.AppointmentDay_Time','df0.ScheduledDay_Hours','df0.ScheduledDay_Date','df0.AppointmentDay_Date','df0.day_diff')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PatientId', 'precedent_appointments']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|precedent_appointments|\n",
      "+----------------------+\n",
      "|                     1|\n",
      "|                     1|\n",
      "|                     2|\n",
      "+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform Feature - 'precedent_appointments' (new)\n",
    "# Create a new feature to tell number of precedent appointments \n",
    "# Keep only the last entry for the patient, hence need to minus 1 appointment from the total \n",
    "\n",
    "# df0.groupby('PatientId').count().show()\n",
    "import pyspark.sql.functions as f\n",
    "appointments = df0.groupBy('PatientId').count().select('PatientId', f.col('count').alias('precedent_appointments'))\n",
    "appointments.columns\n",
    "appointments.select('precedent_appointments').show(3)\n",
    "appointments = appointments.withColumn('precedent_appointments', appointments['precedent_appointments']-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+----------------------+\n",
      "|   PatientId|precedent_appointments|\n",
      "+------------+----------------------+\n",
      "|8.4574399E13|                     0|\n",
      "|4.9823802E12|                     0|\n",
      "|8.6471298E13|                     1|\n",
      "+------------+----------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "appointments.show(3)\n",
    "df0 = df0.join(appointments, how = 'left', on = 'PatientId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PatientId', 'precedent_no_shows']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------------+\n",
      "|   PatientId|precedent_no_shows|\n",
      "+------------+------------------+\n",
      "|8.4574399E13|                -1|\n",
      "|4.9823802E12|                -1|\n",
      "|8.6471298E13|                -1|\n",
      "+------------+------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Transform Feature - 'precedent_no_shows' (new)\n",
    "# Create a new feature to tell number of precedent no-show records \n",
    "# Repeat the step for no-shows\n",
    "\n",
    "# df0.groupBy('PatientId').count().select('PatientId', f.col('count').alias('precedent_appointments'))\n",
    "from pyspark.sql.functions import sum as _sum\n",
    "no_shows = df0.groupBy('PatientId').agg(_sum('No-show').alias('precedent_no_shows'))\n",
    "no_shows = no_shows.withColumn('precedent_no_shows', no_shows['precedent_no_shows']-1)\n",
    "no_shows.columns\n",
    "no_shows.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = df0.join(no_shows, how = 'left', on = 'PatientId')\n",
    "df0 = df0.withColumn(\"precedent_no_shows\",when(df0[\"precedent_no_shows\"] == -1,0).otherwise(df0[\"precedent_no_shows\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PatientId',\n",
       " 'Scholarship',\n",
       " 'Hipertension',\n",
       " 'Diabetes',\n",
       " 'Alcoholism',\n",
       " 'Handcap',\n",
       " 'AppointmentID',\n",
       " 'Gender',\n",
       " 'ScheduledDay',\n",
       " 'AppointmentDay',\n",
       " 'Age',\n",
       " 'Neighbourhood',\n",
       " 'SMS_received',\n",
       " 'No-show',\n",
       " 'ScheduledDay_Day',\n",
       " 'ScheduledDay_Time',\n",
       " 'AppointmentDay_Day',\n",
       " 'AppointmentDay_Time',\n",
       " 'ScheduledDay_Hours',\n",
       " 'ScheduledDay_Date',\n",
       " 'AppointmentDay_Date',\n",
       " 'day_diff',\n",
       " 'precedent_appointments',\n",
       " 'precedent_no_shows']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df0.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----+\n",
      "|       Neighbourhood|count|\n",
      "+--------------------+-----+\n",
      "|   PARQUE INDUSTRIAL|    1|\n",
      "|ILHAS OCE?NICAS D...|    2|\n",
      "|           AEROPORTO|    8|\n",
      "|       ILHA DO FRADE|   10|\n",
      "|         ILHA DO BOI|   35|\n",
      "|   PONTAL DE CAMBURI|   69|\n",
      "|   MORADA DE CAMBURI|   96|\n",
      "|            NAZARETH|  135|\n",
      "|    SEGURAN?A DO LAR|  145|\n",
      "|       UNIVERSIT?RIO|  152|\n",
      "|               HORTO|  175|\n",
      "|        SANTA HELENA|  178|\n",
      "|      ENSEADA DO SU?|  235|\n",
      "|           FRADINHOS|  258|\n",
      "|     ANT?NIO HON?RIO|  271|\n",
      "| ARIOVALDO FAVALESSA|  282|\n",
      "|          DE LOURDES|  305|\n",
      "|             COMDUSA|  310|\n",
      "|           BOA VISTA|  312|\n",
      "|      M?RIO CYPRESTE|  371|\n",
      "+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Too many levels in 'Neighbourhood' variables\n",
    "# Difficult to append additional info about the neigbourhoods\n",
    "print(df0.groupBy('Neighbourhood').count().orderBy('count').show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Duplicate PatientIds, Keep Latest Appointment Entry Only\n",
    "df1 = df0\n",
    "# df1.sort_values('AppointmentDay', inplace = True)\n",
    "# df1.drop_duplicates(subset='PatientId', keep = 'last', inplace = True)\n",
    "# print(df1)\n",
    "# df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110527\n"
     ]
    }
   ],
   "source": [
    "print(df1.count())\n",
    "df1 = df1.drop_duplicates(subset=['PatientId'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = df1.select('Gender', 'Age', 'Scholarship', 'Hipertension', 'Diabetes',\n",
    "       'Alcoholism', 'Handcap', 'SMS_received', 'No-show',\n",
    "       'ScheduledDay_Hours', 'ScheduledDay_Date', 'AppointmentDay_Date',\n",
    "       'day_diff', 'precedent_appointments', \n",
    "       'precedent_no_shows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "61744"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "110527"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.count()\n",
    "df0.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove NA values for variables (age & day_diff) \n",
    "from pyspark.sql.functions import isnan, when, count, col\n",
    "df1 = df1.withColumn('age',F.when(F.col('age').isNull(),0).otherwise(F.col('age')))\n",
    "df1.where(col('age').isNull()).count()\n",
    "\n",
    "df1 = df1.withColumn('day_diff',F.when(F.col('day_diff').isNull(),0).otherwise(F.col('day_diff')))\n",
    "df1.where(col('day_diff').isNull()).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---+-----------+------------+--------+----------+-------+------------+-------+------------------+-----------------+-------------------+--------+----------------------+------------------+\n",
      "|Gender|age|Scholarship|Hipertension|Diabetes|Alcoholism|Handcap|SMS_received|No-show|ScheduledDay_Hours|ScheduledDay_Date|AppointmentDay_Date|day_diff|precedent_appointments|precedent_no_shows|\n",
      "+------+---+-----------+------------+--------+----------+-------+------------+-------+------------------+-----------------+-------------------+--------+----------------------+------------------+\n",
      "|     0|  0|          0|           0|       0|         0|      0|           0|      0|                 0|                0|                  0|       0|                     0|                 0|\n",
      "+------+---+-----------+------------+--------+----------+-------+------------+-------+------------------+-----------------+-------------------+--------+----------------------+------------------+\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Show NA values (if any)\n",
    "print( df1.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in df1.columns]).show() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Gender: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Scholarship: integer (nullable = true)\n",
      " |-- Hipertension: integer (nullable = true)\n",
      " |-- Diabetes: integer (nullable = true)\n",
      " |-- Alcoholism: integer (nullable = true)\n",
      " |-- Handcap: integer (nullable = true)\n",
      " |-- SMS_received: float (nullable = true)\n",
      " |-- No-show: integer (nullable = true)\n",
      " |-- ScheduledDay_Hours: string (nullable = true)\n",
      " |-- ScheduledDay_Date: string (nullable = true)\n",
      " |-- AppointmentDay_Date: string (nullable = true)\n",
      " |-- day_diff: integer (nullable = true)\n",
      " |-- precedent_appointments: long (nullable = true)\n",
      " |-- precedent_no_shows: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Gender: integer (nullable = true)\n",
      " |-- age: integer (nullable = true)\n",
      " |-- Scholarship: integer (nullable = true)\n",
      " |-- Hipertension: integer (nullable = true)\n",
      " |-- Diabetes: integer (nullable = true)\n",
      " |-- Alcoholism: integer (nullable = true)\n",
      " |-- Handcap: integer (nullable = true)\n",
      " |-- SMS_received: integer (nullable = true)\n",
      " |-- No-show: integer (nullable = true)\n",
      " |-- ScheduledDay_Hours: integer (nullable = true)\n",
      " |-- ScheduledDay_Date: integer (nullable = true)\n",
      " |-- AppointmentDay_Date: integer (nullable = true)\n",
      " |-- day_diff: integer (nullable = true)\n",
      " |-- precedent_appointments: integer (nullable = true)\n",
      " |-- precedent_no_shows: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "df1 = df1.withColumn(\"SMS_received\", df1[\"SMS_received\"].cast(IntegerType()))\n",
    "df1 = df1.withColumn(\"ScheduledDay_Hours\", df1[\"ScheduledDay_Hours\"].cast(IntegerType()))\n",
    "df1 = df1.withColumn(\"ScheduledDay_Date\", df1[\"ScheduledDay_Date\"].cast(IntegerType()))\n",
    "df1 = df1.withColumn(\"AppointmentDay_Date\", df1[\"AppointmentDay_Date\"].cast(IntegerType()))\n",
    "df1 = df1.withColumn(\"precedent_appointments\", df1[\"precedent_appointments\"].cast(IntegerType()))\n",
    "df1 = df1.withColumn(\"precedent_no_shows\", df1[\"precedent_no_shows\"].cast(IntegerType()))\n",
    "df1.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Align the column naming convention\n",
    "df1 = df1.withColumnRenamed('Gender', 'gender')\n",
    "df1 = df1.withColumnRenamed('Age', 'age')\n",
    "df1 = df1.withColumnRenamed('Scholarship', 'scholarship')\n",
    "df1 = df1.withColumnRenamed('Hipertension', 'hypertension')\n",
    "df1 = df1.withColumnRenamed('Diabetes', 'diabetes')\n",
    "df1 = df1.withColumnRenamed('Alcoholism', 'alcoholism')\n",
    "df1 = df1.withColumnRenamed('Handcap', 'handicap')\n",
    "df1 = df1.withColumnRenamed('SMS_received', 'sms')\n",
    "df1 = df1.withColumnRenamed('No-show', 'no_show')\n",
    "df1 = df1.withColumnRenamed('ScheduledDay_Hours', 'schedule_hour')\n",
    "df1 = df1.withColumnRenamed('ScheduledDay_Date', 'schedule_day')\n",
    "df1 = df1.withColumnRenamed('AppointmentDay_Date', 'appointment_day')\n",
    "df1 = df1.withColumnRenamed('precedent_no_shows', 'precedent_noshows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender',\n",
       " 'age',\n",
       " 'scholarship',\n",
       " 'hypertension',\n",
       " 'diabetes',\n",
       " 'alcoholism',\n",
       " 'handicap',\n",
       " 'sms',\n",
       " 'no_show',\n",
       " 'schedule_hour',\n",
       " 'schedule_day',\n",
       " 'appointment_day',\n",
       " 'day_diff',\n",
       " 'precedent_appointments',\n",
       " 'precedent_noshows']"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the Processed File\n",
    "# df1.to_pickle('df1.clean')\n",
    "df1.rdd.saveAsPickleFile('./df1.clean')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
